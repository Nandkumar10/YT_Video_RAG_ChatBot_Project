{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85EV9SNPOb3I",
        "outputId": "91ac4b83-9c34-465b-b1ae-79bd40384458"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.2/485.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\"\"\" pip install -q youtube_transcript_api langchain_community langchain-huggingface \\\n",
        "transformers huggingface-hub faiss-cpu tiktoken python-dotenv \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc86zfPwN_Pv",
        "outputId": "49af967d-76ee-42ef-98a3-9249fddda8d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/nandkumaradmane/Desktop/ChatBot_RAG/ChatBotenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEndpointEmbeddings, HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]= \"'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'\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBpZW4R1QZ28",
        "outputId": "efb7c01f-1099-4e20-92c5-443b47bbc225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38\n"
          ]
        }
      ],
      "source": [
        "#from langchain_community.document_loaders import YoutubeLoader\n",
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEndpointEmbeddings, HuggingFaceEndpoint, ChatHuggingFace\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "''' from dotenv import load_dotenv\n",
        "load_dotenv() '''\n",
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]= \"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\"\n",
        "\n",
        "\n",
        "## Step1 Indexing\n",
        "\n",
        "url = \"LPZh9BOjkQs\"\n",
        "try:\n",
        "        Fetched = YouTubeTranscriptApi().fetch(video_id=url, languages=[\"en\"])\n",
        "        transcript_list = Fetched.to_raw_data()\n",
        "\n",
        "        transcript = \" \".join(chunk['text'] for chunk in transcript_list)\n",
        "        #print(transcript)\n",
        "\n",
        "except Exception as e:\n",
        "        print(\"No captions available for this video\")\n",
        "        print(e)\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=250,\n",
        "        chunk_overlap=50\n",
        ")\n",
        "chunks = splitter.create_documents([transcript])\n",
        "print(len(chunks))\n",
        "\n",
        "\n",
        "embedding = HuggingFaceEndpointEmbeddings(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "vector_store = FAISS.from_documents(chunks, embedding)\n",
        "#print(vector_store.index_to_docstore_id)\n",
        "\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={\n",
        "        \"k\": 3,\n",
        "}\n",
        ")\n",
        "\n",
        "#retrieved_doc = retriever.invoke(\"What is LLM\")\n",
        "\n",
        "\"\"\" llm = HuggingFaceEndpoint(repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "        task= \"Text Generation\")\n",
        "model = ChatHuggingFace(llm = llm) \"\"\"\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"conversational\"\n",
        ")\n",
        "\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "        template= \"\"\"\n",
        "You are a helpfull assistent .\n",
        "Ans from the provided transcript context in simple way.\n",
        "If the context is insufficient, just say don't know.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\"\"\",\n",
        "input_variables= ['context', 'question']\n",
        ")\n",
        "question = 'This video is on which topic'\n",
        "retrieved_doc = retriever.invoke(question)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "HAIBcSleqD1O"
      },
      "outputs": [],
      "source": [
        "context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_doc)\n",
        "#context_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzfGJDy3qn8K",
        "outputId": "f35f6143-c5a8-4212-8f2f-1cbd6711d4c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='The video mentioned in the text is about the difference between two fundamental operations and whether to choose one over the other, but the specific topic is not explicitly stated. However, based on the context provided, it seems to be related to artificial intelligence and machine learning, specifically concerning the development of chatbots through reinforcement learning with human feedback. If further clarification is needed, I would suggest watching the video to determine the exact topic. If the context is not sufficient, the assistant cannot determine the specific topic as it is not explicitly stated.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 109, 'prompt_tokens': 226, 'total_tokens': 335}, 'model_name': 'HuggingFaceH4/zephyr-7b-beta', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c5fb6-ca4f-7252-990c-230c4e3f7bf8-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 226, 'output_tokens': 109, 'total_tokens': 335}\n"
          ]
        }
      ],
      "source": [
        "final_prompt = prompt.invoke({\"context\": context_text, \"question\": question})\n",
        "\n",
        "result = model.invoke(final_prompt)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Kc1tVXH_qZo8"
      },
      "outputs": [],
      "source": [
        "def format_docs(retrieved_doc):\n",
        "  context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_doc)\n",
        "  return context_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "qHGIl-Y2wiw8"
      },
      "outputs": [],
      "source": [
        "parallel_chain = RunnableParallel({\n",
        "    'context': retriever | RunnableLambda(format_docs),\n",
        "    'question': RunnablePassthrough()\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_a3oWh_xLhl",
        "outputId": "238e8f45-4eae-4959-b949-14d56851cd49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context': 'A large language model is a sophisticated mathematical function that predicts what word comes next for any piece of text. Instead of predicting one word with certainty, though, what it does is assign a probability to all possible next words. To\\n\\na produced video, but I leave it up to you which one of these feels like the better follow-on.\\n\\nThis operation gives all of these lists of numbers a chance to talk to one another and refine the meanings they encode based on the context around, all done in parallel. For example, the numbers encoding the word bank might be changed based on the',\n",
              " 'question': 'What is an LLM'}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parallel_chain.invoke(\"What is an LLM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "5wh2X07nxR3u",
        "outputId": "4ab8cd01-e5d6-4853-8149-9dc142fe8967"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The provided transcript suggests that the speaker is discussing a large language model, which predicts the probability of the next word in a text sequence instead of just guessing one specific word. They provide two options: either continuing with a specific video or explaining the concept of attention in a series on deep learning. They ask the listener to choose which one they prefer. The summary of the video they recommend is unclear as the context provided does not offer enough information to determine the content. If there is not enough context, the assistant cannot summarize the video.'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parser = StrOutputParser()\n",
        "main_chain = parallel_chain | prompt | model | parser\n",
        "main_chain.invoke(\"Summarize this video\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptJulOmNykuW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
