ğŸ¬ YouTube Video RAG ChatBot Project

An AI-driven chatbot that allows users to ask questions about any YouTube video and receive accurate answers based on the videoâ€™s content using Retrieval-Augmented Generation (RAG) with LangChain and vector embeddings.

ğŸ§  About

This project lets you extract transcripts from YouTube videos, transform them into embeddings in a vector database, and build an intelligent chatbot that can:

âœ” Understand video content
âœ” Answer user questions with context-aware responses
âœ” Provide summaries, explanations, and detailed info from videos

Instead of just feeding the video into an LLM, this system uses RAG to retrieve relevant chunks of the transcript before generating answers, improving accuracy and relevance.

ğŸš€ Features

âœ” Fetch YouTube video transcripts
âœ” Convert transcript text into embeddings
âœ” Store embeddings for fast retrieval
âœ” Perform semantic search over video content
âœ” Answer user questions with detailed responses
âœ” Built using LangChain and vector database

ğŸ§± Tech Stack

Python

LangChain

HuggingFace / LLM API

Vector Database (FAISS/Chroma)

YouTube Transcript Fetch

Streamlit UI (for interactive chat)


ğŸ›  How It Works

User enters a YouTube video link

Video transcript is fetched automatically

Transcript is split into smaller chunks

Each chunk is embedded into a vector store

User asks a question in the UI

Relevant chunks are retrieved using vector similarity

The LLM generates a context-aware answer

ğŸ§ª Example Use Cases

â€œWhat are the key points explained in the video?â€

â€œSummarize the speakerâ€™s recommendations.â€

â€œWhat tool did the presenter mention at minute 8:45?â€

â€œExplain the concept of X from the video.â€


ğŸ¤” Why LangChain Instead of LangGraph?

In this project, LangChain was used instead of LangGraph because the application follows a straightforward RAG pipeline and does not require complex agent workflows.

âœ… Why LangChain Was Chosen

This project involves:

Loading YouTube transcripts

Splitting text into chunks

Creating embeddings

Storing them in a vector database

Retrieving relevant context

Generating responses using an LLM

This is a linear and well-defined workflow, which LangChain handles efficiently using:

Document Loaders

Text Splitters

Embedding Models

Vector Stores

Retrievers

LLM Chains

LangChain provides all the required abstractions for building a clean and modular RAG system without additional orchestration complexity.

ğŸ§  Why LangGraph Was Not Used

LangGraph is more suitable for:

Multi-agent systems

Complex branching logic

Stateful workflows

Tool-calling agents with decision loops

Advanced conversational memory management

Since this project:

Does not require multi-step decision-making

Does not involve multiple agents

Does not need graph-based execution flows

Follows a simple query â†’ retrieve â†’ generate pipeline

Using LangGraph would have added unnecessary architectural complexity.

ğŸ¯ Design Decision

The goal of this project was to:

Demonstrate a clean RAG implementation

Keep the architecture simple and readable

Focus on retrieval accuracy and LLM integration

Make the system beginner-friendly and easy to extend

Therefore, LangChain was the most appropriate and efficient choice for this use case.
